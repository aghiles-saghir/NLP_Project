
# Clustering et Analyse de FrÃ©quences des Mots dans des Documents

Ce projet implÃ©mente un pipeline de traitement de texte pour regrouper des documents en clusters Ã  l'aide de l'algorithme KMeans et analyser les mots frÃ©quents dans chaque cluster.

## ğŸ“‹ FonctionnalitÃ©s

1. **Chargement des donnÃ©es :**
   - Lecture de fichiers JSONL contenant des avis et des mÃ©tadonnÃ©es.

2. **Traitement des textes :**
   - Tokenisation avec spaCy.
   - Lemmatisation des tokens.
   - Suppression des stopwords et nettoyage des tokens.
   - Conversion des textes nettoyÃ©s en une reprÃ©sentation vectorielle (TF-IDF).

3. **Clustering :**
   - Groupement des textes en clusters avec l'algorithme KMeans.
   - Calcul des mots les plus frÃ©quents pour chaque cluster.

4. **Ã‰valuation des clusters :**
   - Calcul du **Silhouette Score** pour mesurer la qualitÃ© des regroupements.

5. **Export des rÃ©sultats :**
   - Sauvegarde des documents enrichis avec leur cluster dans des fichiers JSONL.

## ğŸš€ Installation

1. Clonez ce dÃ©pÃ´t :
   ```bash
   git clone https://github.com/Aghiles-S/NLP_Project.git
   cd NLP_Project
   ```

2. Installez les dÃ©pendances nÃ©cessaires :
   ```bash
   pip install -r requirements.txt
   ```

3. TÃ©lÃ©chargez et installez le modÃ¨le spaCy :
   ```bash
   python -m spacy download en_core_web_sm
   ```

## ğŸ› ï¸ Utilisation

1. Placez vos fichiers JSONL d'entrÃ©e dans le dossier `./data` :
   - `reviews.jsonl` : Contient les avis Ã  analyser.
   - `meta.jsonl` : Contient des mÃ©tadonnÃ©es.

2. ExÃ©cutez le script principal :
   ```bash
   python main.py
   ```

3. Les rÃ©sultats seront sauvegardÃ©s dans le dossier `./processed_data` avec un horodatage.

## ğŸ“Š RÃ©sultats

- Chaque cluster contient les textes groupÃ©s selon leur similaritÃ©.
- Les 10 mots les plus frÃ©quents par cluster sont affichÃ©s dans la console.
- Les donnÃ©es enrichies (clusters, textes nettoyÃ©s) sont sauvegardÃ©es dans un fichier JSONL.

## ğŸ“ˆ Ã‰valuation

- Le **Silhouette Score** est calculÃ© pour Ã©valuer la qualitÃ© des clusters. Ce score est affichÃ© dans la console.

## ğŸ›¡ï¸ Conventions de codage

- Le code utilise Python 3.12.
- Les bibliothÃ¨ques principales sont :
  - `pandas` pour la manipulation des donnÃ©es.
  - `scikit-learn` pour la vectorisation et le clustering.
  - `spaCy` pour le traitement du langage naturel.

## ğŸ“‚ Structure du projet

```plaintext
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reviews.jsonl          # Fichier d'avis (input)
â”‚   â”œâ”€â”€ meta.jsonl             # Fichier de mÃ©tadonnÃ©es (facultatif)
â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ reviews_processed_<date>.jsonl # Fichiers nettoyÃ©s
â”‚   â”œâ”€â”€ reviews_clustered_<date>.jsonl # Fichiers regroupÃ©s par cluster
â”œâ”€â”€ src                  # Script principal
â”‚   â”œâ”€â”€ main.py           # Pipeline de traitement de texte
â”œâ”€â”€ requirements.txt           # DÃ©pendances
â”œâ”€â”€ README.md                  # Documentation
â””â”€â”€ .gitignore                 # Fichiers ignorÃ©s par Git
```

## ğŸ“š Ressources

- **spaCy** : [Documentation officielle](https://spacy.io/)
- **Scikit-learn** : [Documentation officielle](https://scikit-learn.org/)
- **TF-IDF** : [Principe expliquÃ©](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

## ğŸ“ Auteurs

- **Aghiles SAGHIR**
- **Amayas MAHMOUDI**